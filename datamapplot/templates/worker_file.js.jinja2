{# Web Worker code for file-based (non-inline) data mode #}
{# Note: The filename passed to decompressFile already contains the full URL #}
const parsingWorkerBlob = new Blob([`
  self.onmessage = async function(event) {
    const { encodedData, JSONParse } = event.data;
    
    async function decompressFile(filename) {
      try {
        const response = await fetch(filename);
        if (!response.ok) {
          throw new Error(\`HTTP error! status: \${response.status}. Failed to fetch: \${filename}\`);
        }
        const reader = response.body
          .pipeThrough(new DecompressionStream("gzip"))
          .getReader();

        let chunks = [];
        let totalSize = 0;

        while (true) {
          const { done, value } = await reader.read();
          if (done) {
            break;
          }
          chunks.push(value);
          totalSize += value.length;
        }

        // Concatenate chunks into a single Uint8Array
        const decompressedData = new Uint8Array(totalSize);
        let offset = 0;
        for (const chunk of chunks) {
          decompressedData.set(chunk, offset);
          offset += chunk.length;
        }

        return decompressedData;
      } catch (error) {
        console.error('Decompression failed:', error);
        throw error;
      }
    }
    
    let processedCount = 0;
    const decodedData = encodedData.map(async (file, i) => {
      const binaryData = await decompressFile(file);
      processedCount += 1;
      self.postMessage({ type: "progress", progress: Math.round(((processedCount) / encodedData.length) * 95) });
      
      if (JSONParse) {
        const parsedData = JSON.parse(new TextDecoder("utf-8").decode(binaryData));
        return { chunkIndex: i, chunkData: parsedData };
      } else {
        return { chunkIndex: i, chunkData: binaryData };
      }
    });
    
    self.postMessage({ type: "data", data: await Promise.all(decodedData) });
  }
`], { type: 'application/javascript' });
